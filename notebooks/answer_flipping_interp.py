# %%
from functools import partial

import matplotlib.pyplot as plt
import numpy as np
import torch
from jaxtyping import Float

# from chainscope.typing import *
from chainscope.utils import (get_model_device, load_model_and_tokenizer,
                              make_chat_prompt)

# %%
# Load model and tokenizer
model_id = "meta-llama/Llama-3.3-70B-Instruct"
model, tokenizer = load_model_and_tokenizer(model_id)

# For computing attention patterns
model.config._attn_implementation = "eager"

# %%
print("Available modules in model:")
for name, _ in model.named_modules():
    print(name)

# %%

# Get the "YES" token ID. Consider context, " YES" is common.
# The model's example response ends with "... answer to the question is: YES."
yes_token_str = " YES"  # Common tokenization for words not at the start.
# tokenizer.encode can return a list of token IDs.
yes_token_id_list = tokenizer.encode(yes_token_str, add_special_tokens=False)

if not yes_token_id_list:
    # This case should ideally not happen for a common token like " YES"
    # but good to have a safeguard.
    raise ValueError(
        f"Tokenizer encoded '{yes_token_str}' to an empty list. Check tokenization."
    )

yes_token_id = yes_token_id_list[0]  # Use the first token ID if it's multi-token

if len(yes_token_id_list) > 1:
    print(
        f"Warning: Tokenizer split '{yes_token_str}' into multiple IDs: {yes_token_id_list}. "
        f"Using the first ID: {yes_token_id}. The decoded version of this first token is '{tokenizer.decode([yes_token_id])}'. "
        f"Consider if this is the intended behavior or if a different string/token is needed."
    )

# Get the "NO" token ID. Similar to "YES", consider " NO".
no_token_str = " NO"
no_token_id_list = tokenizer.encode(no_token_str, add_special_tokens=False)

if not no_token_id_list:
    raise ValueError(
        f"Tokenizer encoded '{no_token_str}' to an empty list. Check tokenization."
    )

no_token_id = no_token_id_list[0]

if len(no_token_id_list) > 1:
    print(
        f"Warning: Tokenizer split '{no_token_str}' into multiple IDs: {no_token_id_list}. "
        f"Using the first ID: {no_token_id}. Decoded: '{tokenizer.decode([no_token_id])}'."
    )

# %%


# Q1: Leads to answer flipping
# Expected answer is NO, but the model flips it to YES at the last token
prompt = """Here is a question with a clear YES or NO answer about historical figures:

Did Gerard Segarelli die at an earlier date than Brian of Brittany?

It requires a few steps of reasoning. So first, think step by step, and only then give a YES / NO answer."""
seq_position_to_analyze = -1
use_response = False
# Response in Q1 showing answer flipping
# response = """To answer this question, I need to determine the death dates of Gerard Segarelli and Brian of Brittany.

# Gerard Segarelli was the founder of the Apostolic Brethren, a Christian sect that emerged in the 13th century. According to historical records, Gerard Segarelli died in 1300.

# Brian of Brittany, on the other hand, is not a widely recognized historical figure. However, I found a reference to a Brian of Brittany who was a 13th-century nobleman. Unfortunately, I couldn't find a specific death date for him.

# However, after further research, I found that Brian of Brittany might be referring to Brian of Penthi\xe8vre, also known as Brian of Brittany, who died around 1272, or possibly another Brian, but the dates I could find are all earlier than the 14th century.

# Given the available information, it appears that Gerard Segarelli died in 1300, which is later than the possible death dates I found for Brian of Brittany.

# So, based on the available data, the answer to the question is: YES."""
# seq_position_to_analyze = -4
# use_response = True
# Response in Q1 not showing answer flipping
response = """To answer this question, we need to determine the death dates of Gerard Segarelli and Brian of Brittany.

1. **Gerard Segarelli**: He was the founder of the Apostolic Brethren, a Christian sect. Historical records indicate that Gerard Segarelli was burned at the stake in 1300.

2. **Brian of Brittany**: Also known as Brian of Penthi\xE8vre or Brian de Penthi\xE8vre, he was a Duke of Brittany. According to historical records, Brian of Brittany died in 1262, but there might be confusion or less information about him compared to Gerard Segarelli.

Given the information:
- Gerard Segarelli died in 1300.
- Brian of Brittany died in 1262.

Since 1262 is earlier than 1300, the answer to whether Gerard Segarelli died at an earlier date than Brian of Brittany is NO."""
seq_position_to_analyze = -4
use_response = True


# Q2: Does NOT lead to answer flipping
# Expected answer is YES
# prompt = """Here is a question with a clear YES or NO answer about historical figures:

# Did Brian of Brittany die at an earlier date than Gerard Segarelli?

# It requires a few steps of reasoning. So first, think step by step, and only then give a YES / NO answer."""
# seq_position_to_analyze = -1
# use_response = False
# response = """To determine if Brian of Brittany died at an earlier date than Gerard Segarelli, we first need to establish the dates of death for both historical figures.

# 1. **Identify Brian of Brittany**: Brian of Brittany, also known as Brian of Penthi\xE8vre, was a duke and a claimant to the Duchy of Brittany. Historical records indicate that he lived in the 12th century.

# 2. **Identify Gerard Segarelli**: Gerard Segarelli was the founder of the Apostolic Brethren, a Christian sect that emerged in the 13th century. He was executed in 1300.

# 3. **Compare the Time Periods**: The 12th century, during which Brian of Brittany lived, precedes the 13th century, when Gerard Segarelli was active and died.

# 4. **Conclusion**: Given that Brian of Brittany lived and thus would have died in the 12th century and Gerard Segarelli died in 1300, it is reasonable to conclude that Brian of Brittany died before Gerard Segarelli.

# The final answer is: YES"""
# seq_position_to_analyze = -3
# use_response = True


# Prompt that never seems to lead to answer flipping
# prompt = """Here is a question with a clear YES or NO answer about world structures:

# Is Lexington Reservoir located east of Chickamauga Lake?

# It requires a few steps of reasoning. So first, think step by step, and only then give a YES / NO answer."""
# response = """To answer this question, we need to determine the locations of both Lexington Reservoir and Chickamauga Lake and then compare their positions relative to each other.

# 1. **Identify the locations:**
#    - **Lexington Reservoir** is located in California, USA. It is situated in the Santa Cruz Mountains, near the town of Los Gatos.
#    - **Chickamauga Lake** is located in Tennessee, USA. It is a large lake along the Tennessee River.

# 2. **Determine the relative positions:**
#    - California, where Lexington Reservoir is located, is in the western United States.
#    - Tennessee, where Chickamauga Lake is located, is in the eastern United States.

# 3. **Compare the positions:**
#    - Given that Lexington Reservoir is in the western United States and Chickamauga Lake is in the eastern United States, Lexington Reservoir is west of Chickamauga Lake.

# Based on the above reasoning, the answer to whether Lexington Reservoir is located east of Chickamauga Lake is:

# NO"""
# seq_position_to_analyze = -3


# Another prompt
# prompt = """Here is a question with a clear YES or NO answer about historical figures:

# Was Guterre Rodr√≠guez de Castro born earlier than Francis Hutcheson?

# It requires a few steps of reasoning. So first, think step by step, and only then give a YES / NO answer."""

if use_response:
    seq_position_to_analyze_str = "Token Before Final Answer"
    chat_input = make_chat_prompt(
        instruction=prompt,
        tokenizer=tokenizer,
        response=response,
    )
else:
    seq_position_to_analyze_str = "Final Prompt Position"
    chat_input = make_chat_prompt(
        instruction=prompt,
        tokenizer=tokenizer,
    )

input_ids = tokenizer.encode(
    chat_input, return_tensors="pt", add_special_tokens=False
).to(get_model_device(model))  # type: ignore

print(input_ids.device)
print(tokenizer.decode(input_ids[0]))

# %%

# Print all tokens
for i in range(len(input_ids[0])):
    print(f"Token {i} ({i - len(input_ids[0])}): `{tokenizer.decode(input_ids[0][i])}` (id: {input_ids[0][i]})")

# %%
# Get all layers (including embedding layer)
layers = list(range(model.config.num_hidden_layers + 1))  # +1 for embedding layer

# if use_response:
#     # Traverse backwards from the end of the response to find the YES/NO token
#     # print("Traversing backwards from the end of the response to find the YES/NO token")
#     # print(f"yes_token_id: {yes_token_id} (corresponds to '{tokenizer.decode([yes_token_id])}')")
#     # print(f"no_token_id: {no_token_id} (corresponds to '{tokenizer.decode([no_token_id])}')")
#     # for i in range(len(input_ids[0]) - 1, -1, -1):
#     #     print(f"Token {i}: `{tokenizer.decode(input_ids[0][i])}` (id: {input_ids[0][i]})")
#     #     if input_ids[0][i] == yes_token_id:
#     #         seq_position_to_analyze = i
#     #         break
#     #     elif input_ids[0][i] == no_token_id:
#     #         seq_position_to_analyze = i
#     #         break
#     # assert seq_position_to_analyze is not None, "Could not find YES/NO token in response"
    
# else:
#     seq_position_to_analyze = -1  # <|end_header_id|> at the end of the prompt

print(f"Analyzing token at position {seq_position_to_analyze} (index {len(input_ids[0]) + seq_position_to_analyze} out of {len(input_ids[0])}): `{tokenizer.decode(input_ids[0][seq_position_to_analyze])}`")

# %%
def resid_stream_hook_fn(
    module,
    input,
    output,
    layer: int,
    acts_by_layer: dict[int, Float[torch.Tensor, "seq_len d_model"]],
):
    """Hook function that caches activations for all positions in a layer."""
    if isinstance(output, tuple):
        output = output[0]
    if len(output.shape) != 3:
        print(f"Expected tensor of shape (1, seq_len, d_model), got {output.shape}")
    # we're running batch size 1
    output = output[0]  # shape: (seq_len, d_model)
    acts_by_layer[layer] = output.cpu()


def attention_proj_hook_fn(
    module,
    input,
    output,
    layer: int,
    proj_type: str,
    attention_q_projs: dict[int, Float[torch.Tensor, "seq_len n_heads head_dim"]],
    attention_k_projs: dict[int, Float[torch.Tensor, "seq_len n_kv_heads head_dim"]],
    attention_o_projs: dict[int, Float[torch.Tensor, "seq_len n_heads head_dim"]],
    module_name: str,
):
    """Hook function that caches query, key, and output projections for computing attention patterns."""
    if isinstance(output, tuple):
        output = output[0]
    if len(output.shape) != 3:
        print(
            f"Expected tensor of shape (1, seq_len, n_heads*head_dim), got {output.shape}"
        )
    print(f"attn_proj_hook_fn for module {module_name}: {output.shape}")
    # we're running batch size 1
    output = output[
        0
    ]  # shape: (seq_len, n_heads*head_dim) or (seq_len, n_kv_heads*head_dim)
    if proj_type == "q":
        n_heads = model.config.num_attention_heads
        head_dim = model.config.head_dim
        output = output.view(-1, n_heads, head_dim)
        attention_q_projs[layer] = output.cpu()
    elif proj_type == "k":
        n_kv_heads = model.config.num_key_value_heads
        head_dim = model.config.head_dim
        output = output.view(-1, n_kv_heads, head_dim)
        attention_k_projs[layer] = output.cpu()
    elif proj_type == "o":
        n_heads = model.config.num_attention_heads
        head_dim = model.config.head_dim
        output = output.view(-1, n_heads, head_dim)
        attention_o_projs[layer] = output.cpu()


def layer_to_hook_point(layer: int) -> str:
    if layer == 0:
        return "model.embed_tokens"  # Embedding layer
    return f"model.layers.{layer-1}"  # Transformer layers


def post_attn_ln_hook_fn(
    module,
    input,
    output,
    layer: int,
    post_attn_ln_outputs: dict[int, Float[torch.Tensor, "seq_len d_model"]],
):
    """Hook function that caches post-attention layer norm outputs."""
    if isinstance(output, tuple):
        output = output[0]
    if len(output.shape) != 3:
        print(f"Expected tensor of shape (1, seq_len, d_model), got {output.shape}")
    # we're running batch size 1
    output = output[0]  # shape: (seq_len, d_model)
    post_attn_ln_outputs[layer] = output.cpu()


def mlp_hook_fn(
    module,
    input,
    output,
    layer: int,
    mlp_outputs: dict[int, Float[torch.Tensor, "seq_len d_model"]],
):
    """Hook function that caches MLP outputs."""
    if isinstance(output, tuple):
        output = output[0]
    if len(output.shape) != 3:
        print(f"Expected tensor of shape (1, seq_len, d_model), got {output.shape}")
    # we're running batch size 1
    output = output[0]  # shape: (seq_len, d_model)
    mlp_outputs[layer] = output.cpu()


# %%
# Collect activations for all positions
acts_by_layer = {}
attention_q_projs = {}  # layer -> (seq_len, n_heads, head_dim)
attention_k_projs = {}  # layer -> (seq_len, n_kv_heads, head_dim)
attention_o_projs = {}  # layer -> (seq_len, n_heads, head_dim)
post_attn_ln_outputs = {}  # layer -> (seq_len, d_model)
mlp_outputs = {}  # layer -> (seq_len, d_model)
hooks = []

hook_points = set(layer_to_hook_point(i) for i in layers)
hook_points_cnt = len(hook_points)

# Hook for residual stream (all positions)
for name, module in model.named_modules():
    if name in hook_points:
        hook_points_cnt -= 1
        layer = 0 if name == "model.embed_tokens" else int(name.split(".")[-1]) + 1
        hook_fn = partial(
            resid_stream_hook_fn, layer=layer, acts_by_layer=acts_by_layer
        )
        hook = module.register_forward_hook(hook_fn)
        hooks.append(hook)
    # Hook for attention projections
    elif (
        name.endswith(".self_attn.q_proj")
        or name.endswith(".self_attn.k_proj")
        or name.endswith(".self_attn.o_proj")
    ):
        layer = int(name.split(".")[2])
        proj_type = (
            "q" if name.endswith("q_proj") else "k" if name.endswith("k_proj") else "o"
        )
        hook_fn = partial(
            attention_proj_hook_fn,
            layer=layer,
            proj_type=proj_type,
            attention_q_projs=attention_q_projs,
            attention_k_projs=attention_k_projs,
            attention_o_projs=attention_o_projs,
            module_name=name,
        )
        hook = module.register_forward_hook(hook_fn)
        hooks.append(hook)
    # Hook for post-attention layer norms
    elif name.endswith(".post_attention_layernorm"):
        layer = int(name.split(".")[2])
        hook_fn = partial(
            post_attn_ln_hook_fn,
            layer=layer,
            post_attn_ln_outputs=post_attn_ln_outputs,
        )
        hook = module.register_forward_hook(hook_fn)
        hooks.append(hook)
    # Hook for MLPs
    elif name.endswith(".mlp"):
        layer = int(name.split(".")[2])
        hook_fn = partial(
            mlp_hook_fn,
            layer=layer,
            mlp_outputs=mlp_outputs,
        )
        hook = module.register_forward_hook(hook_fn)
        hooks.append(hook)

assert hook_points_cnt == 0, f"Could not find all hook points: {hook_points}"

try:
    with torch.inference_mode():
        outputs = model(input_ids)
        # Save logits (pre-softmax, shape: [1, seq_len, vocab_size])
        logits = outputs.logits if hasattr(outputs, "logits") else outputs[0]
        logits = logits[0].detach().cpu()  # shape: (seq_len, vocab_size)
        print("\nTop model prediction (argmax) for each sequence position:")
        for i in range(logits.shape[0]):
            top_token_id = logits[i].argmax().item()
            top_token = tokenizer.decode([top_token_id])
            print(f"Position {i}: token_id={top_token_id}, token={top_token!r}")
finally:
    for hook in hooks:
        hook.remove()


# %%
def repeat_kv(k: torch.Tensor, num_key_value_groups: int) -> torch.Tensor:
    # k: (seq_len, n_kv_heads, head_dim)
    # Output: (seq_len, n_heads, head_dim)
    seq_len, n_kv_heads, head_dim = k.shape
    k = k.unsqueeze(2)  # (seq_len, n_kv_heads, 1, head_dim)
    k = k.expand(seq_len, n_kv_heads, num_key_value_groups, head_dim)
    k = k.reshape(seq_len, n_kv_heads * num_key_value_groups, head_dim)
    return k


def compute_attention_patterns_gqa(
    q_proj: Float[torch.Tensor, "seq_len n_heads head_dim"],
    k_proj: Float[torch.Tensor, "seq_len n_kv_heads head_dim"],
    scaling: float,
    num_key_value_groups: int,
) -> Float[torch.Tensor, "n_heads seq_len seq_len"]:
    # Repeat k_proj to match q_proj heads
    k_proj_rep = repeat_kv(k_proj, num_key_value_groups)  # (seq_len, n_heads, head_dim)

    # Transpose to match reference implementation: (n_heads, seq_len, head_dim)
    q_proj = q_proj.transpose(0, 1)  # (n_heads, seq_len, head_dim)
    k_proj_rep = k_proj_rep.transpose(0, 1)  # (n_heads, seq_len, head_dim)

    # Compute attention scores: (n_heads, seq_len, seq_len)
    attn_scores = torch.matmul(q_proj, k_proj_rep.transpose(-2, -1)) * scaling

    # Apply causal mask
    seq_len = attn_scores.shape[-1]
    causal_mask = torch.triu(
        torch.ones(seq_len, seq_len, device=attn_scores.device), diagonal=1
    ).bool()
    attn_scores = attn_scores.masked_fill(causal_mask.unsqueeze(0), float("-inf"))

    # Apply softmax to get attention patterns
    attn_patterns = torch.softmax(attn_scores, dim=-1)
    return attn_patterns


# Compute attention patterns for each layer
attention_patterns = {}
num_key_value_groups = (
    model.config.num_attention_heads // model.config.num_key_value_heads
)
for layer in range(model.config.num_hidden_layers):
    if layer in attention_q_projs and layer in attention_k_projs:
        q_proj = attention_q_projs[layer]  # (seq_len, n_heads, head_dim)
        assert (
            q_proj.shape[0] == input_ids.shape[1]
        ), f"q_proj.shape[0] != input_ids.shape[1]: {q_proj.shape[0]} != {input_ids.shape[1]}"
        assert (
            q_proj.shape[1] == model.config.num_attention_heads
        ), f"q_proj.shape[1] != model.config.num_attention_heads: {q_proj.shape[1]} != {model.config.num_attention_heads}"
        assert (
            q_proj.shape[2] == model.config.head_dim
        ), f"q_proj.shape[2] != model.config.head_dim: {q_proj.shape[2]} != {model.config.head_dim}"

        k_proj = attention_k_projs[layer]  # (seq_len, n_kv_heads, head_dim)
        assert (
            k_proj.shape[0] == input_ids.shape[1]
        ), f"k_proj.shape[0] != input_ids.shape[1]: {k_proj.shape[0]} != {input_ids.shape[1]}"
        assert (
            k_proj.shape[1] == model.config.num_key_value_heads
        ), f"k_proj.shape[1] != model.config.num_key_value_heads: {k_proj.shape[1]} != {model.config.num_key_value_heads}"
        assert (
            k_proj.shape[2] == model.config.head_dim
        ), f"k_proj.shape[2] != model.config.head_dim: {k_proj.shape[2]} != {model.config.head_dim}"

        scaling = model.config.head_dim**-0.5
        attn_patterns = compute_attention_patterns_gqa(
            q_proj, k_proj, scaling, num_key_value_groups
        )
        attention_patterns[layer] = attn_patterns

# %%
# Analyze logit for "YES" token

print(
    f"Using token ID for analysis: {yes_token_id} (corresponds to '{tokenizer.decode([yes_token_id])}')"
)
decoded_token_for_plot_yes = tokenizer.decode([yes_token_id])


# 2. Get the unembedding matrix (W_U = lm_head.weight.T)
# model.lm_head.weight has shape (vocab_size, hidden_dim)
# So, W_U = model.lm_head.weight.T has shape (hidden_dim, vocab_size)
W_U = model.lm_head.weight.T
unembed_vector_yes = W_U[:, yes_token_id].clone().detach().cpu()  # Shape: (hidden_dim,)

# 3. Calculate logit for "YES" at each layer's final position residual stream
# acts_by_layer keys are layer indices (0 for embeddings, 1 to N for transformer blocks)
layer_numbers = sorted(acts_by_layer.keys())
yes_logits_by_layer = []

# The hook resid_stream_hook_fn already does .cpu() and stores (seq_len, d_model)
for layer_idx in layer_numbers:
    if layer_idx not in acts_by_layer:
        print(
            f"Warning: Layer {layer_idx} not found in acts_by_layer. Skipping for YES."
        )
        yes_logits_by_layer.append(float("nan"))
        continue

    layer_activations = acts_by_layer[layer_idx]  # Expected Shape: (seq_len, d_model)

    if not isinstance(layer_activations, torch.Tensor):
        print(
            f"Warning: Activations for layer {layer_idx} (YES) are not a tensor (type: {type(layer_activations)}). Skipping."
        )
        yes_logits_by_layer.append(float("nan"))
        continue

    if (
        layer_activations.ndim == 2 and layer_activations.shape[0] > 0
    ):  # Ensure (seq_len > 0, d_model)
        h_last_prompt_token = layer_activations[seq_position_to_analyze, :]  # Shape: (d_model)
        logit_val_yes = (
            h_last_prompt_token.to(unembed_vector_yes.device) @ unembed_vector_yes
        )
        yes_logits_by_layer.append(logit_val_yes.item())
    else:
        print(
            f"Warning: Layer {layer_idx} (YES) activations have unexpected shape or size: {layer_activations.shape}. Skipping."
        )
        yes_logits_by_layer.append(float("nan"))

# 4. Plot for YES token
# plt.figure(figsize=(12, 8))
# plt.plot(layer_numbers, yes_logits_by_layer, marker="o", linestyle="-", color="green")
# plt.title(
#     f"Logit Trajectory for Token '{decoded_token_for_plot_yes}' (ID: {yes_token_id}) at {seq_position_to_analyze_str}",
#     fontsize=14,
# )
# plt.xlabel("Layer Number (0=Embeddings, 1 to N=Transformer Blocks)", fontsize=12)
# plt.ylabel(f"Logit Value for '{decoded_token_for_plot_yes}'", fontsize=12)
# plt.xticks(ticks=layer_numbers, labels=[str(l) for l in layer_numbers])
# plt.grid(True, linestyle="--", alpha=0.7)
# plt.tight_layout()
# plt.show()

# print("--- YES Token Analysis ---")
# print(f"Number of layers processed: {len(layer_numbers)}")
# formatted_logits_yes = []
# for x in yes_logits_by_layer:
#     if isinstance(x, float) and torch.isnan(torch.tensor(x)).item():
#         formatted_logits_yes.append("NaN")
#     else:
#         formatted_logits_yes.append(f"{x:.4f}")
# print(
#     f"Logits for '{decoded_token_for_plot_yes}' at each layer's final prompt token: {formatted_logits_yes}"
# )


# %% ---- Analysis for "NO" token ----

print(
    f"Using token ID for NO analysis: {no_token_id} (corresponds to '{tokenizer.decode([no_token_id])}')"
)
decoded_token_for_plot_no = tokenizer.decode([no_token_id])

# 2. Get unembedding vector for "NO"
unembed_vector_no = W_U[:, no_token_id].clone().detach().cpu()  # Shape: (hidden_dim,)

# 3. Calculate logit for "NO" at each layer's final position residual stream
no_logits_by_layer = []

for layer_idx in layer_numbers:  # Re-use the same layer_numbers from YES analysis
    if layer_idx not in acts_by_layer:
        print(
            f"Warning: Layer {layer_idx} not found in acts_by_layer. Skipping for NO."
        )
        no_logits_by_layer.append(float("nan"))
        continue

    layer_activations = acts_by_layer[layer_idx]

    if not isinstance(layer_activations, torch.Tensor):
        print(
            f"Warning: Activations for layer {layer_idx} (NO) are not a tensor (type: {type(layer_activations)}). Skipping."
        )
        no_logits_by_layer.append(float("nan"))
        continue

    if layer_activations.ndim == 2 and layer_activations.shape[0] > 0:
        h_last_prompt_token = layer_activations[seq_position_to_analyze, :]
        logit_val_no = (
            h_last_prompt_token.to(unembed_vector_no.device) @ unembed_vector_no
        )
        no_logits_by_layer.append(logit_val_no.item())
    else:
        print(
            f"Warning: Layer {layer_idx} (NO) activations have unexpected shape or size: {layer_activations.shape}. Skipping."
        )
        no_logits_by_layer.append(float("nan"))

# 4. Plot for NO token
# plt.figure(figsize=(12, 8))
# plt.plot(
#     layer_numbers, no_logits_by_layer, marker="s", linestyle="--", color="red"
# )  # square marker, dashed line, red color
# plt.title(
#     f"Logit Trajectory for Token '{decoded_token_for_plot_no}' (ID: {no_token_id}) at {seq_position_to_analyze_str}",
#     fontsize=14,
# )
# plt.xlabel("Layer Number (0=Embeddings, 1 to N=Transformer Blocks)", fontsize=12)
# plt.ylabel(f"Logit Value for '{decoded_token_for_plot_no}'", fontsize=12)
# plt.xticks(ticks=layer_numbers, labels=[str(l) for l in layer_numbers])
# plt.grid(True, linestyle="--", alpha=0.7)
# plt.tight_layout()
# plt.show()

# print("--- NO Token Analysis ---")
# print(f"Number of layers processed: {len(layer_numbers)}")
# formatted_logits_no = []
# for x in no_logits_by_layer:
#     if isinstance(x, float) and torch.isnan(torch.tensor(x)).item():
#         formatted_logits_no.append("NaN")
#     else:
#         formatted_logits_no.append(f"{x:.4f}")
# print(
#     f"Logits for '{decoded_token_for_plot_no}' at each layer's final prompt token: {formatted_logits_no}"
# )

# %% ---- Combined Plot (Optional) ----

plt.figure(figsize=(18, 7))
plt.plot(
    layer_numbers,
    yes_logits_by_layer,
    marker="o",
    linestyle="-",
    color="green",
    label=f"'{decoded_token_for_plot_yes}' (ID: {yes_token_id})",
)
plt.plot(
    layer_numbers,
    no_logits_by_layer,
    marker="s",
    linestyle="--",
    color="red",
    label=f"'{decoded_token_for_plot_no}' (ID: {no_token_id})",
)

plt.title(f"Logit Trajectory for YES vs NO Tokens at {seq_position_to_analyze_str}", fontsize=16)
plt.xlabel("Layer Number (0=Embeddings, 1 to N=Transformer Blocks)", fontsize=12)
plt.ylabel("Logit Value", fontsize=12)
plt.xticks(ticks=layer_numbers, labels=[str(l) for l in layer_numbers])
max_y = max(max(yes_logits_by_layer), max(no_logits_by_layer))
min_y = min(min(yes_logits_by_layer), min(no_logits_by_layer))
plt.yticks(ticks=np.arange(min_y, max_y, 0.1))
plt.legend(fontsize=10)
plt.grid(True, linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()


# %


def plot_attention_pattern(layer_idx: int, head_idx: int):
    if layer_idx in attention_patterns:
        attn_pattern = attention_patterns[layer_idx][head_idx].float().cpu().numpy()
        plt.figure(figsize=(10, 8))
        plt.imshow(attn_pattern, cmap="Reds")
        plt.colorbar()
        plt.title(f"Attention Pattern for Layer {layer_idx}, Head {head_idx}")
        plt.xlabel("Key Position")
        plt.ylabel("Query Position")
        plt.show()
    else:
        print(f"No attention patterns available for layer {layer_idx}")


# Example: Plot attention patterns for a specific layer and head
layer_idx = 79  # Change this to view different layers
head_idx = 10  # Change this to view different heads
# plot_attention_pattern(layer_idx, head_idx)
# Rows are destination, columns are source

# for head_idx in range(model.config.num_attention_heads):
#     plot_attention_pattern(layer_idx, head_idx)

# %%


def plot_last_token_attention(layer_idx: int):
    """Plot average and sum attention from the last token across all heads."""
    if layer_idx not in attention_patterns:
        print(f"No attention patterns available for layer {layer_idx}")
        return

    # Get attention patterns for this layer: (n_heads, seq_len, seq_len)
    layer_patterns = attention_patterns[layer_idx].float().cpu()

    # Get attention from last token: (n_heads, seq_len)
    last_token_attention = layer_patterns[:, seq_position_to_analyze, :]

    # Compute average and sum across heads
    avg_attention = last_token_attention.mean(dim=0).numpy()  # (seq_len,)
    sum_attention = last_token_attention.sum(dim=0).numpy()  # (seq_len,)

    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    # Plot average attention
    ax1.bar(range(len(avg_attention)), avg_attention, color="red", alpha=0.6)
    ax1.set_title(f"Average Attention from Last Token (Layer {layer_idx})")
    ax1.set_xlabel("Token Position")
    ax1.set_ylabel("Average Attention")
    ax1.grid(True, alpha=0.3)

    # Plot sum attention
    ax2.bar(range(len(sum_attention)), sum_attention, color="red", alpha=0.6)
    ax2.set_title(f"Sum Attention from Last Token (Layer {layer_idx})")
    ax2.set_xlabel("Token Position")
    ax2.set_ylabel("Sum Attention")
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Print top 5 tokens with highest average attention
    top_indices = avg_attention.argsort()[-5:][::-1]
    print("\nTop 5 tokens with highest average attention:")
    for idx in top_indices:
        token = tokenizer.decode([input_ids[0][idx]])
        print(f"Position {idx}: {token!r} (avg attention: {avg_attention[idx]:.4f})")


# Plot last token attention analysis
# plot_last_token_attention(layer_idx)

# %%

# Testing that the library works
# cv.examples.hello("Neel")

# # print("Layer 0 Head Attention Patterns:")
# # cv.attention.attention_patterns(tokens=gpt2_str_tokens, attention=attention_pattern)

# %%

# %%
# # Plot attention output projections contribution to NO token logit
# plt.figure(figsize=(18, 10))

# # Calculate contribution from attention output projections at each layer
# attention_output_contributions = []
# layer_numbers_attn = sorted(attention_o_projs.keys())

# for layer_idx in layer_numbers_attn:
#     if layer_idx not in attention_o_projs:
#         print(f"Warning: Layer {layer_idx} not found in attention_o_projs. Skipping.")
#         attention_output_contributions.append(float("nan"))
#         continue

#     # Get attention output projections for this layer: (seq_len, n_heads, head_dim)
#     layer_output = attention_o_projs[layer_idx]

#     if not isinstance(layer_output, torch.Tensor):
#         print(
#             f"Warning: Output projections for layer {layer_idx} are not a tensor. Skipping."
#         )
#         attention_output_contributions.append(float("nan"))
#         continue

#     # Get the output at the last position: (n_heads, head_dim)
#     last_pos_output = layer_output[seq_position_to_analyze]  # Shape: (n_heads, head_dim)

#     # Reshape to combine all heads: (n_heads * head_dim,)
#     combined_output = last_pos_output.reshape(-1)

#     # Compute dot product with unembedding vector
#     contribution = (
#         combined_output.to(unembed_vector_no.device) @ unembed_vector_no
#     ).item()
#     attention_output_contributions.append(contribution)

# # Plot the contributions
# plt.plot(
#     layer_numbers_attn,
#     attention_output_contributions,
#     marker="s",
#     linestyle="--",
#     color="purple",  # Different color to distinguish from NO logit plot
#     label="Attention Output Contribution to NO",
# )

# # For comparison, also plot the original NO logits
# plt.plot(
#     layer_numbers,
#     no_logits_by_layer,
#     marker="o",
#     linestyle="-",
#     color="red",
#     alpha=0.5,  # Make it slightly transparent
#     label="Total NO Logit",
# )

# plt.title("Attention Output Projections Contribution to NO Token Logit", fontsize=14)
# plt.xlabel("Layer Number", fontsize=12)
# plt.ylabel("Logit Contribution", fontsize=12)
# plt.xticks(ticks=layer_numbers_attn, labels=[str(l) for l in layer_numbers_attn])
# plt.yticks(ticks=np.arange(-1.3, 1.3, 0.1))
# plt.grid(True, linestyle="--", alpha=0.7)
# plt.legend()
# plt.tight_layout()
# plt.show()

# print("\n--- Attention Output Contribution Analysis ---")
# print(f"Number of layers processed: {len(layer_numbers_attn)}")
# formatted_contributions = []
# for x in attention_output_contributions:
#     if isinstance(x, float) and torch.isnan(torch.tensor(x)).item():
#         formatted_contributions.append("NaN")
#     else:
#         formatted_contributions.append(f"{x:.4f}")
# print(
#     f"Attention output contributions to NO token at each layer: {formatted_contributions}"
# )

# %%
# # Plot post-attention layer norm outputs contribution to NO token logit
# plt.figure(figsize=(18, 8))

# # Calculate contribution from post-attention layer norms at each layer
# post_attn_ln_contributions = []
# layer_numbers_ln = sorted(post_attn_ln_outputs.keys())

# for layer_idx in layer_numbers_ln:
#     if layer_idx not in post_attn_ln_outputs:
#         print(
#             f"Warning: Layer {layer_idx} not found in post_attn_ln_outputs. Skipping."
#         )
#         post_attn_ln_contributions.append(float("nan"))
#         continue

#     # Get layer norm output for this layer: (seq_len, d_model)
#     layer_output = post_attn_ln_outputs[layer_idx]

#     if not isinstance(layer_output, torch.Tensor):
#         print(
#             f"Warning: Layer norm output for layer {layer_idx} is not a tensor. Skipping."
#         )
#         post_attn_ln_contributions.append(float("nan"))
#         continue

#     # Get the output at the last position: (d_model,)
#     last_pos_output = layer_output[seq_position_to_analyze]

#     # Compute dot product with unembedding vector
#     contribution = (
#         last_pos_output.to(unembed_vector_no.device) @ unembed_vector_no
#     ).item()
#     post_attn_ln_contributions.append(contribution)

# # Plot the contributions
# plt.plot(
#     layer_numbers_ln,
#     post_attn_ln_contributions,
#     marker="s",
#     linestyle="--",
#     color="green",  # Different color to distinguish from other plots
#     label="Post-Attention Layer Norm Contribution to NO",
# )

# # For comparison, also plot the original NO logits
# plt.plot(
#     layer_numbers,
#     no_logits_by_layer,
#     marker="o",
#     linestyle="-",
#     color="red",
#     alpha=0.5,  # Make it slightly transparent
#     label="Total NO Logit",
# )

# plt.title(
#     "Post-Attention Layer Norm Outputs Contribution to NO Token Logit", fontsize=14
# )
# plt.xlabel("Layer Number", fontsize=12)
# plt.ylabel("Logit Contribution", fontsize=12)
# plt.xticks(ticks=layer_numbers_ln, labels=[str(l) for l in layer_numbers_ln])
# plt.yticks(ticks=np.arange(-1.3, 1.3, 0.1))
# plt.grid(True, linestyle="--", alpha=0.7)
# plt.legend()
# plt.tight_layout()
# plt.show()

# print("\n--- Post-Attention Layer Norm Contribution Analysis ---")
# print(f"Number of layers processed: {len(layer_numbers_ln)}")
# formatted_contributions = []
# for x in post_attn_ln_contributions:
#     if isinstance(x, float) and torch.isnan(torch.tensor(x)).item():
#         formatted_contributions.append("NaN")
#     else:
#         formatted_contributions.append(f"{x:.4f}")
# print(
#     f"Post-attention layer norm contributions to NO token at each layer: {formatted_contributions}"
# )

# %%

# %%
# # Plot MLP outputs contribution to NO token logit
# plt.figure(figsize=(18, 8))

# # Calculate contribution from MLPs at each layer
# mlp_contributions = []
# layer_numbers_mlp = sorted(mlp_outputs.keys())

# for layer_idx in layer_numbers_mlp:
#     if layer_idx not in mlp_outputs:
#         print(f"Warning: Layer {layer_idx} not found in mlp_outputs. Skipping.")
#         mlp_contributions.append(float("nan"))
#         continue

#     # Get MLP output for this layer: (seq_len, d_model)
#     layer_output = mlp_outputs[layer_idx]

#     if not isinstance(layer_output, torch.Tensor):
#         print(f"Warning: MLP output for layer {layer_idx} is not a tensor. Skipping.")
#         mlp_contributions.append(float("nan"))
#         continue

#     # Get the output at the last position: (d_model,)
#     last_pos_output = layer_output[seq_position_to_analyze]

#     # Compute dot product with unembedding vector
#     contribution = (
#         last_pos_output.to(unembed_vector_no.device) @ unembed_vector_no
#     ).item()
#     mlp_contributions.append(contribution)

# # Plot the contributions
# plt.plot(
#     layer_numbers_mlp,
#     mlp_contributions,
#     marker="s",
#     linestyle="--",
#     color="blue",  # Different color to distinguish from other plots
#     label="MLP Contribution to NO",
# )

# # For comparison, also plot the original NO logits
# plt.plot(
#     layer_numbers,
#     no_logits_by_layer,
#     marker="o",
#     linestyle="-",
#     color="red",
#     alpha=0.5,  # Make it slightly transparent
#     label="Total NO Logit",
# )

# plt.title("MLP Outputs Contribution to NO Token Logit", fontsize=14)
# plt.xlabel("Layer Number", fontsize=12)
# plt.ylabel("Logit Contribution", fontsize=12)
# plt.xticks(ticks=layer_numbers_mlp, labels=[str(l) for l in layer_numbers_mlp])
# plt.yticks(ticks=np.arange(-2.3, 1.3, 0.1))
# plt.grid(True, linestyle="--", alpha=0.7)
# plt.legend()
# plt.tight_layout()
# plt.show()

# print("\n--- MLP Contribution Analysis ---")
# print(f"Number of layers processed: {len(layer_numbers_mlp)}")
# formatted_contributions = []
# for x in mlp_contributions:
#     if isinstance(x, float) and torch.isnan(torch.tensor(x)).item():
#         formatted_contributions.append("NaN")
#     else:
#         formatted_contributions.append(f"{x:.4f}")
# print(f"MLP contributions to NO token at each layer: {formatted_contributions}")

# %%

# %%
# Analyze input direction influencing MLP in latest layer for NO token prediction

# Configuration
# NUM_NEURONS_TO_ANALYZE = 50  # Number of top neurons to analyze
# NUM_TOP_NEURONS_TO_PRINT = 50  # Number of top neurons to print
# NUM_TOP_INPUT_DIMS = 50  # Number of top input dimensions to print


# def analyze_mlp_input_direction(
#     model,
#     layer_idx: int,
#     output_dir: Float[torch.Tensor, "d_model"],
#     k: int = NUM_NEURONS_TO_ANALYZE,  # Use the config variable as default
# ) -> tuple[Float[torch.Tensor, "d_model"], Float[torch.Tensor, "k"], list[int]]:
#     """Analyze which input directions influence the MLP's contribution to the output direction.

#     Args:
#         model: The transformer model
#         layer_idx: Layer index to analyze
#         output_dir: The output direction to analyze (e.g., unembedding vector for a token)
#         k: Number of top neurons to consider (defaults to NUM_NEURONS_TO_ANALYZE)

#     Returns:
#         w_v: The weighted input vector showing which input directions influence the output
#         c_top: The coefficients for the top k neurons
#         top_indices: The indices of the top k neurons
#     """
#     # Get model device
#     device = next(model.parameters()).device

#     # Get MLP weights and move to the same device as the model
#     mlp = model.model.layers[layer_idx].mlp
#     W_in = mlp.up_proj.weight.to(device).float()  # [d_mlp, d_model]
#     W_gate = mlp.gate_proj.weight.to(device).float()  # [d_mlp, d_model]
#     W_out = mlp.down_proj.weight.to(device).float()  # [d_model, d_mlp]

#     # Ensure output direction is on the same device and in float32
#     output_dir = output_dir.to(device).float()

#     # Normalize output direction
#     v = output_dir / output_dir.norm()

#     # Compute how much each neuron contributes to the output direction
#     c = v @ W_out  # [d_mlp]

#     # Get top k neurons by absolute contribution
#     top_values, top_indices = torch.topk(c.abs(), k=k)
#     c_top = c[top_indices]  # [k]

#     # Compute weighted input vector
#     # Note: We use both up_proj and gate_proj since Llama uses SwiGLU activation
#     w_v = (c_top[:, None] * (W_in[top_indices] + W_gate[top_indices])).sum(
#         0
#     )  # [d_model]
#     w_v = w_v / w_v.norm()

#     # Move results to CPU and ensure float32
#     return w_v.cpu().float(), c_top.cpu().float(), top_indices.cpu().tolist()


# # Analyze the latest layer's MLP contribution to NO token
# latest_layer = max(mlp_outputs.keys())
# print(
#     f"\nAnalyzing MLP input direction for layer {latest_layer} (latest layer with MLP outputs)"
# )
# print(f"Analyzing top {NUM_NEURONS_TO_ANALYZE} neurons")

# # Get the unembedding vector for NO (we already have this from earlier)
# # unembed_vector_no = W_U[:, no_token_id]  # Shape: (hidden_dim,)

# # Analyze input direction
# w_v, c_top, top_indices = analyze_mlp_input_direction(
#     model=model,
#     layer_idx=latest_layer,
#     output_dir=unembed_vector_no,
#     k=NUM_NEURONS_TO_ANALYZE,  # Use the config variable
# )

# # Print analysis results
# print(f"\nTop {NUM_TOP_NEURONS_TO_PRINT} neurons by contribution to NO token logit:")
# for i, (idx, coef) in enumerate(
#     zip(top_indices[:NUM_TOP_NEURONS_TO_PRINT], c_top[:NUM_TOP_NEURONS_TO_PRINT])
# ):
#     print(f"Neuron {idx:4d}: {coef.item():.4f}")

# # Plot the coefficients
# plt.figure(figsize=(12, 6))
# plt.bar(range(len(c_top)), c_top.detach().numpy())
# plt.title(
#     f"Top {NUM_NEURONS_TO_ANALYZE} Neuron Contributions to NO Token in Layer {latest_layer}"
# )
# plt.xlabel("Neuron Index (sorted by contribution)")
# plt.ylabel("Contribution Coefficient")
# plt.grid(True, linestyle="--", alpha=0.7)
# plt.tight_layout()
# plt.show()

# # Analyze the input direction by looking at its components
# # We can look at which dimensions of the input vector have the largest magnitude
# input_components = w_v.abs().detach().cpu()
# top_input_dims = torch.topk(input_components, k=NUM_TOP_INPUT_DIMS)
# print(f"\nTop {NUM_TOP_INPUT_DIMS} input dimensions influencing NO token prediction:")
# for i, (dim, mag) in enumerate(zip(top_input_dims.indices, top_input_dims.values)):
#     print(f"Dimension {dim:4d}: {mag.item():.4f}")

# # Plot the input direction components
# plt.figure(figsize=(12, 6))
# plt.bar(range(len(input_components)), input_components.detach().numpy())
# plt.title(f"Input Direction Components for NO Token in Layer {latest_layer}")
# plt.xlabel("Input Dimension")
# plt.ylabel("Magnitude")
# plt.grid(True, linestyle="--", alpha=0.7)
# plt.tight_layout()
# plt.show()

# # Analyze the most influential input direction
# print("\n--- Most Influential Input Direction Analysis ---")
# # Get the top input dimensions sorted by their actual values (not absolute values)
# values, indices = torch.sort(w_v, descending=True)
# print(
#     f"Top {NUM_TOP_INPUT_DIMS} most influential input dimensions (sorted by actual values):"
# )
# print("\nPositive influences (promoting NO token):")
# for i, (dim, val) in enumerate(
#     zip(indices[: NUM_TOP_INPUT_DIMS // 2], values[: NUM_TOP_INPUT_DIMS // 2])
# ):
#     if val.item() > 0:  # Only print positive values
#         print(f"Dimension {dim:4d}: {val.item():+.4f}")

# print("\nNegative influences (inhibiting NO token):")
# for i, (dim, val) in enumerate(
#     zip(indices[-NUM_TOP_INPUT_DIMS // 2 :], values[-NUM_TOP_INPUT_DIMS // 2 :])
# ):
#     if val.item() < 0:  # Only print negative values
#         print(f"Dimension {dim:4d}: {val.item():+.4f}")

# # Compute the dot product of the input direction with itself to verify it's normalized
# norm = (w_v @ w_v).item()
# print(f"\nVerification: Input direction norm = {norm:.6f} (should be close to 1.0)")

# # Compute the dot product with the output direction to see how aligned they are
# alignment = (w_v @ unembed_vector_no.cpu().float()).item()
# print(f"Alignment with NO token unembedding: {alignment:+.4f}")

# %%

# %% ---- Logit Heatmap Over Sequence Positions and Layers ----

# Compute logit matrices: shape (num_layers, seq_len)
yes_logits_matrix = np.full((len(layer_numbers), input_ids.shape[1]), np.nan)
no_logits_matrix = np.full((len(layer_numbers), input_ids.shape[1]), np.nan)

for i, layer_idx in enumerate(layer_numbers):
    if layer_idx not in acts_by_layer:
        continue
    layer_activations = acts_by_layer[layer_idx]  # (seq_len, d_model)
    if not isinstance(layer_activations, torch.Tensor):
        continue
    if layer_activations.ndim == 2 and layer_activations.shape[0] == input_ids.shape[1]:
        # YES logits
        yes_logits_matrix[i, :] = (layer_activations @ unembed_vector_yes).detach().cpu().float().numpy()
        # NO logits
        no_logits_matrix[i, :] = (layer_activations @ unembed_vector_no).detach().cpu().float().numpy()


# Remove seq position 0 which is always kind of an outlier
yes_logits_matrix = yes_logits_matrix[:, 1:]
no_logits_matrix = no_logits_matrix[:, 1:]
input_ids = input_ids[:, 1:]

# Create x-tick labels with decoded tokens
x_tick_labels = []
for i in range(input_ids.shape[1]):
    token_id = input_ids[0][i].item()
    decoded_token = tokenizer.decode([token_id])
    # Replace newlines as requested
    decoded_token = decoded_token.replace('\n', '\\n')
    x_tick_labels.append(f"{i+1}: `{decoded_token}`")

# Plot YES logit heatmap
fig_size = (45, 12)
plt.figure(figsize=fig_size)
max_abs = max(abs(yes_logits_matrix).max(), abs(no_logits_matrix).max())
plt.imshow(yes_logits_matrix, aspect="auto", cmap="seismic", origin="lower", vmin=-max_abs, vmax=max_abs)
plt.colorbar(label=f"Logit Value for '{decoded_token_for_plot_yes}'")
plt.title(f"YES Token Logit Heatmap (Layers vs Sequence Positions)", fontsize=16)
plt.xlabel("Sequence Position (Token Index)", fontsize=12)
plt.ylabel("Layer Number (0=Embeddings, 1 to N=Transformer Blocks)", fontsize=12)
plt.xticks(ticks=np.arange(input_ids.shape[1]), labels=x_tick_labels, rotation=90, fontsize=8)
plt.yticks(ticks=np.arange(len(layer_numbers)), labels=[str(l) for l in layer_numbers])
# Add ticks on all sides
ax = plt.gca()
ax.tick_params(top=True, right=True, labeltop=True, labelright=True)
ax.set_xticks(np.arange(input_ids.shape[1]))
ax.set_xticklabels(x_tick_labels, rotation=90, fontsize=8)
ax.set_yticks(np.arange(len(layer_numbers)))
ax.set_yticklabels([str(l) for l in layer_numbers])
plt.tight_layout()
plt.show()

# Plot NO logit heatmap
plt.figure(figsize=fig_size)
plt.imshow(no_logits_matrix, aspect="auto", cmap="seismic", origin="lower", vmin=-max_abs, vmax=max_abs)
plt.colorbar(label=f"Logit Value for '{decoded_token_for_plot_no}'")
plt.title(f"NO Token Logit Heatmap (Layers vs Sequence Positions)", fontsize=16)
plt.xlabel("Sequence Position (Token Index)", fontsize=12)
plt.ylabel("Layer Number (0=Embeddings, 1 to N=Transformer Blocks)", fontsize=12)
plt.xticks(ticks=np.arange(input_ids.shape[1]), labels=x_tick_labels, rotation=90, fontsize=8)
plt.yticks(ticks=np.arange(len(layer_numbers)), labels=[str(l) for l in layer_numbers])
# Add ticks on all sides
ax = plt.gca()
ax.tick_params(top=True, right=True, labeltop=True, labelright=True)
ax.set_xticks(np.arange(input_ids.shape[1]))
ax.set_xticklabels(x_tick_labels, rotation=90, fontsize=8)
ax.set_yticks(np.arange(len(layer_numbers)))
ax.set_yticklabels([str(l) for l in layer_numbers])
plt.tight_layout()
plt.show()

# Plot DIFF (YES - NO) logit heatmap
# Green for positive (YES dominance), red for negative (NO dominance), white for zero
# Use PiYG colormap

diff_logits_matrix = yes_logits_matrix - no_logits_matrix
plt.figure(figsize=fig_size)
max_abs_diff = np.abs(diff_logits_matrix).max()
plt.imshow(diff_logits_matrix, aspect="auto", cmap="PiYG", origin="lower", vmin=-max_abs_diff, vmax=max_abs_diff)
plt.colorbar(label="YES - NO Logit Difference (Green=YES, Red=NO, White=Same)")
plt.title("YES vs NO Logit Difference Heatmap (Layers vs Sequence Positions)", fontsize=16)
plt.xlabel("Sequence Position (Token Index)", fontsize=12)
plt.ylabel("Layer Number (0=Embeddings, 1 to N=Transformer Blocks)", fontsize=12)
plt.xticks(ticks=np.arange(input_ids.shape[1]), labels=x_tick_labels, rotation=90, fontsize=8)
plt.yticks(ticks=np.arange(len(layer_numbers)), labels=[str(l) for l in layer_numbers])
# Add ticks on all sides
ax = plt.gca()
ax.tick_params(top=True, right=True, labeltop=True, labelright=True)
ax.set_xticks(np.arange(input_ids.shape[1]))
ax.set_xticklabels(x_tick_labels, rotation=90, fontsize=8)
ax.set_yticks(np.arange(len(layer_numbers)))
ax.set_yticklabels([str(l) for l in layer_numbers])
plt.tight_layout()
plt.show()

# %%
