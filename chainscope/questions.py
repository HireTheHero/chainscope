import hashlib
import logging

from chainscope.ambiguous_qs_eval import evaluate_single_question
from chainscope.typing import *


def make_yes_no_question_pair(
    template: str,
    open_ended_template: str,
    yes_question_by_qid: dict[str, Question],
    no_question_by_qid: dict[str, Question],
    x_name: str,
    y_name: str,
    x_value: int | float,
    y_value: int | float,
):
    """Generate a pair of complementary YES/NO questions by swapping the order of compared items.

    For each pair of items (x,y), generates:
    - YES question comparing x to y
    - NO question comparing y to x
    Questions are stored in the provided dictionaries keyed by their SHA256 hash.

    Args:
        template: Question template with {x} and {y} placeholders
        open_ended_template: Open-ended version of the question template
        yes_question_by_qid: Dict to store YES questions
        no_question_by_qid: Dict to store NO questions
        x_name: Name of the first item
        y_name: Name of the second item
        x_value: Value of the first item
        y_value: Value of the second item
    """
    # Generate YES question (x compared to y)
    yes_q_str = template.format(x=x_name, y=y_name)
    yes_q_str_open_ended = open_ended_template.format(x=x_name, y=y_name)
    yes_qid = hashlib.sha256(yes_q_str.encode()).hexdigest()
    yes_question_by_qid[yes_qid] = Question(
        q_str=yes_q_str,
        q_str_open_ended=yes_q_str_open_ended,
        x_name=x_name,
        y_name=y_name,
        x_value=x_value,
        y_value=y_value,
    )

    # Generate NO question (y compared to x)
    no_q_str = template.format(x=y_name, y=x_name)
    no_q_str_open_ended = open_ended_template.format(x=y_name, y=x_name)
    no_qid = hashlib.sha256(no_q_str.encode()).hexdigest()
    no_question_by_qid[no_qid] = Question(
        q_str=no_q_str,
        q_str_open_ended=no_q_str_open_ended,
        x_name=y_name,
        y_name=x_name,
        x_value=y_value,
        y_value=x_value,
    )


def make_yes_no_questions_datasets(
    *,
    prop_id: str,
    comparison: Literal["gt", "lt"],
    max_comparisons: int,
    template: str,
    open_ended_template: str,
    small_large_pairs: list[tuple[tuple[str, int | float], tuple[str, int | float]]],
    dataset_suffix: str | None,
    remove_ambiguous: bool,
) -> tuple[QsDataset, QsDataset]:
    yes_question_by_qid = {}
    no_question_by_qid = {}
    for (small_name, small_value), (large_name, large_value) in small_large_pairs:
        if comparison == "lt":
            x_name, y_name = small_name, large_name
            x_value, y_value = small_value, large_value
        else:
            x_name, y_name = large_name, small_name
            x_value, y_value = large_value, small_value
        make_yes_no_question_pair(
            template,
            open_ended_template,
            yes_question_by_qid,
            no_question_by_qid,
            x_name=x_name,
            y_name=y_name,
            x_value=x_value,
            y_value=y_value,
        )

    yes_dataset = QsDataset(
        question_by_qid=yes_question_by_qid,
        params=DatasetParams(
            prop_id=prop_id,
            comparison=comparison,
            answer="YES",
            max_comparisons=max_comparisons,
            suffix=dataset_suffix,
        ),
    )

    no_dataset = QsDataset(
        question_by_qid=no_question_by_qid,
        params=DatasetParams(
            prop_id=prop_id,
            comparison=comparison,
            answer="NO",
            max_comparisons=max_comparisons,
            suffix=dataset_suffix,
        ),
    )
    return yes_dataset, no_dataset


def gen_qs(
    prop_id: str,
    n: int,
    max_comparisons: int,
    entity_popularity_filter: int | None,
    min_percent_value_diff: float | None,
    dataset_suffix: str | None,
    remove_ambiguous: bool,
) -> dict[tuple[Literal["gt", "lt"], Literal["YES", "NO"]], QsDataset]:
    """Generate comparative questions for a given property.

    For each comparison type (greater than 'gt' and less than 'lt'), generates n pairs of YES/NO questions.
    The questions are generated by:
    1. Filtering entities by how well-known they are if entity_popularity_filter is set
    2. Sorting all values for the property
    3. Creating pairs of items where one value is greater than the other
    4. Taking n evenly spaced pairs to ensure good coverage of the value range
    5. For each pair, generating both YES and NO questions by swapping the order

    Args:
        prop_id: ID of the property to generate questions for
        n: Total number of question pairs to generate for each comparison type
        max_comparisons: Maximum number of comparisons to generate for each item

    Returns:
        Dictionary mapping (comparison, answer) pairs to generated datasets
    """
    properties = Properties.load(prop_id)
    logging.info(f"Generating {n} questions for {prop_id}")
    logging.info(f"We have {len(properties.value_by_name)} entities for {prop_id}")

    if entity_popularity_filter is not None:
        try:
            prop_eval = PropEval.load_id(prop_id)
            properties.value_by_name = {
                entity_name: entity_value
                for entity_name, entity_value in properties.value_by_name.items()
                if prop_eval.popularity_by_entity_name[entity_name]
                >= entity_popularity_filter
            }
            assert (
                len(properties.value_by_name) > 1
            ), f"Not enough entities left after filtering by popularity of {entity_popularity_filter} for {prop_id}"
            logging.info(f"After filtering by popularity, we have {len(properties.value_by_name)} entities for {prop_id}")
        except FileNotFoundError:
            raise ValueError(
                f"Entity popularity filter set to {entity_popularity_filter} but prop eval not found for {prop_id}"
            )

    # Sort values and split into evenly sized buckets
    all_sorted_values = sorted(properties.value_by_name.items(), key=lambda x: x[1])
    
    # Calculate the value range if we need to filter by percentage difference
    if min_percent_value_diff is not None:
        min_val = all_sorted_values[0][1]
        max_val = all_sorted_values[-1][1]
        value_range = max_val - min_val
        min_absolute_diff = value_range * (min_percent_value_diff / 100)
        logging.info(f"Value range: {value_range}, minimum required difference: {min_absolute_diff}")
    
    small_large_pairs = []
    for small_idx, (small_name, small_value) in enumerate(all_sorted_values):
        logging.info(f"Generating questions for entity `{small_name}` ({small_value}), index {small_idx}/{len(all_sorted_values)}")

        # Track how many comparisons we've added for this small value
        comparisons_for_small = 0
        start_idx = small_idx + 1
        
        # Continue looking at larger values until we hit max_comparisons or run out of values
        for large_idx, (large_name, large_value) in enumerate(all_sorted_values[start_idx:]):
            logging.info(f"Comparing {small_name} ({small_value}) and {large_name} ({large_value}), index {large_idx}/{len(all_sorted_values) - start_idx}")

            if comparisons_for_small >= max_comparisons:
                break
                
            if small_value == large_value:
                logging.info(
                    f"Skipping {small_name} and {large_name} because values are equal ({small_value})"
                )
                continue
                
            # Skip pairs that don't meet the minimum percentage difference requirement
            if min_percent_value_diff is not None:
                if abs(large_value - small_value) < min_absolute_diff:
                    logging.info(
                        f"Skipping {small_name} ({small_value}) and {large_name} ({large_value}) "
                        f"because difference ({abs(large_value - small_value)}) is less than "
                        f"minimum required ({min_absolute_diff})"
                    )
                    continue

            if remove_ambiguous:
                # Evaluating only with the gt template, since this is quite slow otherwise
                q_str = properties.gt_question.format(x=small_name, y=large_name)
                ambiguous_eval_label, ambiguous_eval_analysis = evaluate_single_question(
                    q_str=q_str,
                    evaluator_model_id="gpt-4o",
                    sampling_params=SamplingParams(
                        temperature=0.7,
                        max_new_tokens=1000,
                        top_p=0.9,
                    ),
                    num_evals=5,
                    short_circuit_on_ambiguous=True,
                )
                if ambiguous_eval_label == "AMBIGUOUS":
                    logging.info(f"Skipping question `{q_str}` because it is ambiguous: {ambiguous_eval_analysis}")
                    continue

            logging.info(f"Adding question comparing {small_name} ({small_value}) and {large_name} ({large_value}) to the dataset")
            small_large_pairs.append(
                ((small_name, small_value), (large_name, large_value))
            )
            comparisons_for_small += 1

    total_pairs = len(small_large_pairs)
    if total_pairs < n:
        logging.warning(
            f"Not enough pairs ({total_pairs}) to generate {n} questions. "
            f"Consider lowering min_percent_value_diff (currently {min_percent_value_diff}%) "
            f"or increasing max_comparisons (currently {max_comparisons})"
            f"or decreasing the popularity filter (currently {entity_popularity_filter})"
        )
        n = total_pairs

    # Take evenly spaced indices
    step = total_pairs / n
    indices = [int(i * step) for i in range(n)]
    assert len(set(indices)) == n, "Indices are not unique"
    small_large_pairs = [small_large_pairs[i] for i in indices]

    # Generate datasets for each comparison type and answer
    datasets = {}
    for comparison in ["gt", "lt"]:
        template = (
            properties.gt_question if comparison == "gt" else properties.lt_question
        )
        open_ended_template = (
            properties.gt_open_ended_question
            if comparison == "gt"
            else properties.lt_open_ended_question
        )
        yes_dataset, no_dataset = make_yes_no_questions_datasets(
            prop_id=prop_id,
            comparison=comparison,  # type: ignore
            max_comparisons=max_comparisons,
            template=template,
            open_ended_template=open_ended_template,
            small_large_pairs=small_large_pairs,
            dataset_suffix=dataset_suffix,
            remove_ambiguous=remove_ambiguous,
        )
        datasets[(comparison, "YES")] = yes_dataset  # type: ignore
        datasets[(comparison, "NO")] = no_dataset  # type: ignore

    return datasets
