import anthropic
import time
import os

# --- Claude class (adapted from your project structure) ---
class Claude:
    API_MAX_RETRY = 5
    API_RETRY_SLEEP = 1
    API_ERROR_OUTPUT = "$ERROR$"

    def __init__(self, model_name):
        # The Anthropic client automatically picks up the ANTHROPIC_API_KEY
        # environment variable.
        try:
            self.client = anthropic.Anthropic()
        except anthropic.APIConnectionError as e:
            print(f"Failed to initialize Anthropic client. Ensure ANTHROPIC_API_KEY is set.")
            print(f"Error: {e}")
            raise
        self.model_name = model_name

    def claude_query(self, system_prompt, user_prompt, assistant_prompt, max_tokens=150, temperature=1.0):
        """
        Calls the Anthropic API with a user prompt and an optional prefilled assistant prompt.

        Args:
            system_prompt (str): The system prompt.
            user_prompt (str): The user's prompt.
            assistant_prompt (str): The prefilled text for the assistant. The model will continue from here.
            max_tokens (int): The maximum number of tokens to generate *after* the assistant_prompt.
            temperature (float): The sampling temperature.

        Returns:
            str: The text generated by the model, continuing from the assistant_prompt.
                 Returns API_ERROR_OUTPUT if an error occurs after retries.
        """
        for i_retry in range(self.API_MAX_RETRY):
            try:
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": user_prompt,
                            }
                        ]
                    },
                ]
                # If assistant_prompt is provided, add it as the last message.
                # The model will then continue generating from this point.
                if assistant_prompt: # Check if assistant_prompt is not empty or None
                    messages.append(
                        {
                            "role": "assistant",
                            "content": [
                                {
                                    "type": "text",
                                    "text": assistant_prompt,
                                }
                            ]
                        }
                    )
                
                # Ensure system_prompt is only passed if it's not empty
                request_params = {
                    "model": self.model_name,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "messages": messages
                }
                if system_prompt:
                    request_params["system"] = system_prompt

                response = self.client.messages.create(**request_params)
                
                # The response.content[0].text will contain ONLY the newly generated tokens
                # that follow the prefilled assistant_prompt.
                return response.content[0].text
            
            except anthropic.APIError as e:
                print(f"Anthropic API Error (attempt {i_retry + 1}/{self.API_MAX_RETRY}): {type(e)} {e}")
                if i_retry < self.API_MAX_RETRY - 1:
                    time.sleep(self.API_RETRY_SLEEP * (i_retry + 1)) # Exponential backoff might be better
                else:
                    print("Max retries reached.")
        return self.API_ERROR_OUTPUT

# --- Example Usage ---
if __name__ == "__main__":
    # Ensure the ANTHROPIC_API_KEY environment variable is set.
    # You can set it in your shell: export ANTHROPIC_API_KEY="sk-ant-..."
    if not os.getenv("ANTHROPIC_API_KEY"):
        print("Error: The ANTHROPIC_API_KEY environment variable is not set.")
        print("Please set it before running the script.")
        exit(1)

    # Choose a Claude model.
    # For a list of available models, refer to Anthropic documentation
    # or your project's model choices like "claude-3-opus-20240229",
    # "claude-3-sonnet-20240229", "claude-3-haiku-20240307",
    # "claude-3-5-sonnet-20240620", "claude-2.1", etc.
    claude_model_name = "claude-3-haiku-20240307" 

    try:
        claude_client = Claude(model_name=claude_model_name)
    except Exception as e:
        print(f"Exiting due to client initialization error: {e}")
        exit(1)

    # --- Scenario 1: Simple Prefill ---
    print("\n--- Scenario 1: Simple Prefill ---")
    system_prompt_text = "You are a poetic assistant, skilled in creative writing."
    user_prompt_text = "Continue this famous line: 'To be, or not to be...'"
    # This is where we "put words in its mouth"
    assistant_prefill_text = "To be, or not to be, that is the question; Whether 'tis nobler in the mind to suffer"

    print(f"System Prompt: {system_prompt_text}")
    print(f"User Prompt: {user_prompt_text}")
    print(f"Assistant Prefill: {assistant_prefill_text}")

    model_continuation = claude_client.claude_query(
        system_prompt=system_prompt_text,
        user_prompt=user_prompt_text,
        assistant_prompt=assistant_prefill_text,
        max_tokens=50,
        temperature=0.7
    )

    if model_continuation != Claude.API_ERROR_OUTPUT:
        print("\nModel's Continuation:")
        print(model_continuation)
        print("\nFull Assistant Response (Prefill + Continuation):")
        print(assistant_prefill_text + model_continuation)
    else:
        print("\nFailed to get a response from the API for Scenario 1.")

    # --- Scenario 2: Prefill to guide a story ---
    print("\n\n--- Scenario 2: Prefill to guide a story ---")
    user_prompt_text_story = "Tell me a short story about a detective solving a mystery in a futuristic city."
    # Guide the story's beginning
    assistant_prefill_story = "Detective Kaito navigated the neon-drenched skyways of Neo-Kyoto. His latest case: the disappearance of a renowned scientist. The only clue was a cryptic data chip. Kaito knew this was just the beginning, and the trail would surely lead him to"

    print(f"User Prompt: {user_prompt_text_story}")
    print(f"Assistant Prefill: {assistant_prefill_story}")
    
    model_continuation_story = claude_client.claude_query(
        system_prompt="", # No system prompt for this one
        user_prompt=user_prompt_text_story,
        assistant_prompt=assistant_prefill_story,
        max_tokens=100,
        temperature=0.8
    )

    if model_continuation_story != Claude.API_ERROR_OUTPUT:
        print("\nModel's Continuation:")
        print(model_continuation_story)
        print("\nFull Assistant Response (Prefill + Continuation):")
        print(assistant_prefill_story + model_continuation_story)
    else:
        print("\nFailed to get a response from the API for Scenario 2.")

